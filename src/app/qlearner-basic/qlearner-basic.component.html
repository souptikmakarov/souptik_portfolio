<div class="page-container">
  <h1>Q-Learning with Dyna-Q</h1>

  <h2>1 Overview</h2>

  <p>
    A very simple implementation of the Q-Learning and Dyna-Q solutions to the
    reinforcement learning problem.
  </p>
  <p>This implementation is tested on a 2D grid robot world.</p>

  <h2>Dyna-Q Concept</h2>

  <p>
    Along with the implementation of basic q-learner, this project also implements
    the Dyna technique which is used to "hallucinate" experience based on past
    interactions with the world so that the learner can learn with lesser number
    of interactions with the "real world".
  </p>

  <p>
    The Dyna technique is an extension to the standard Q-learning algorithm
    developed by Richard Sutton in the field of Reinforcement Learning. The idea
    behind Dyna is to augment the standard Q-learning algorithm with a simulated
    experience of the environment. This helps in improving the learning speed and
    efficiency of the algorithm.
  </p>

  <p>
    The basic idea behind Dyna is to create a model of the environment based on
    the agent's experience. This model can then be used to generate simulated
    experience that can be used to update the agent's Q-values. The agent can then
    use the updated Q-values to make better decisions in the actual environment.
  </p>

  <p>There are two main components of the Dyna technique:</p>
  <ol>
    <li>
      Model-based learning: In model-based learning, the agent builds a model of
      the environment based on its experience. This model includes the transition
      probabilities between states and the expected rewards associated with each
      action. The agent can then use this model to simulate experience and update
      its Q-values.
    </li>
    <li>
      Planning: In planning, the agent uses the model to generate simulated
      experience and updates its Q-values based on the simulated experience (dynamic hallucinations). The
      agent can then use the updated Q-values to make better decisions in the
      actual environment.
    </li>
  </ol>

  <p>
    The Dyna algorithm combines model-based learning and planning to improve the
    learning speed and efficiency of the standard Q-learning algorithm. By
    incorporating simulated experience, the agent can learn more quickly from its
    experience and make better decisions in the actual environment.
  </p>

  <h2>Test Environment</h2>
  The navigation task takes place in a 10 x 10 grid world. The particular
  environment is expressed in a CSV file of integers, where the value in each
  position is interpreted as follows:
  <ul>
    <li>blank space.</li>
    <li>Obstacle.</li>
    <li>Starting location for the robot.</li>
    <li>Goal location.</li>
    <li>Quicksand (huge negative reward).</li>
  </ul>

  <p>A sample world looks like this</p>

  <img src="assets/qlearner_world_format_{{themeService.theme}}.png">

  <p style="margin-top: 50px">Below are the results of different training setups</p>

  <div class="run-result-charts">
    <div class="chart-image-container">
      <img class="chart-image" src="assets/dyna100_result_plot_{{themeService.theme}}.png">
      <p class="chart-description">Agents run <span style="color: rgb(199, 23, 23)">without simulated experience</span> outperform ones run <b style="color: rgb(0, 128, 0)">with simulated experience</b> by an average of 2 steps.</p>
    </div>
    <div class="chart-image-container">
      <img class="chart-image" src="assets/dyna200_result_plot_{{themeService.theme}}.png">
      <p class="chart-description">Agents run <b style="color: rgb(0, 128, 0)">with simulated experience</b> outperform ones run <span style="color: rgb(199, 23, 23)">without simulated experience</span> by an average of 2.4 steps.</p>
    </div>
    <div class="chart-image-container">
      <img class="chart-image" src="assets/dyna300_result_plot_{{themeService.theme}}.png">
      <p class="chart-description">Agents run <b style="color: rgb(0, 128, 0)">with simulated experience</b> outperform ones run <span style="color: rgb(199, 23, 23)">without simulated experience</span> by an average of 0.2 steps.</p>
    </div>
  </div>

  <p style="margin-top: 40px;">The balance between the number of runs of simulated experience and the learning rate of the model 
  needs to be determined by tuning the parameters and experimentaion.</p>

</div>