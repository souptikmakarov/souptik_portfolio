<h1>Q-Learning with Dyna-Q</h1>

<h2>1 Overview</h2>

<p>
  A very simple implementation of the Q-Learning and Dyna-Q solutions to the
  reinforcement learning problem.
</p>
<p>This implementation is tested on a 2D grid robot world.</p>

<h2>Dyna-Q Concept</h2>

<p>
  Along with the implementation of basic q-learner, this project also implements
  the Dyna technique which is used to "hallucinate" experience based on past
  interactions with the world so that the learner can learn with lesser number
  of interactions with the "real world".
</p>

<p>
  The Dyna technique is an extension to the standard Q-learning algorithm
  developed by Richard Sutton in the field of Reinforcement Learning. The idea
  behind Dyna is to augment the standard Q-learning algorithm with a simulated
  experience of the environment. This helps in improving the learning speed and
  efficiency of the algorithm.
</p>

<p>
  The basic idea behind Dyna is to create a model of the environment based on
  the agent's experience. This model can then be used to generate simulated
  experience that can be used to update the agent's Q-values. The agent can then
  use the updated Q-values to make better decisions in the actual environment.
</p>

<p>There are two main components of the Dyna technique:</p>
<ol>
  <li>
    Model-based learning: In model-based learning, the agent builds a model of
    the environment based on its experience. This model includes the transition
    probabilities between states and the expected rewards associated with each
    action. The agent can then use this model to simulate experience and update
    its Q-values.
  </li>
  <li>
    Planning: In planning, the agent uses the model to generate simulated
    experience and updates its Q-values based on the simulated experience. The
    agent can then use the updated Q-values to make better decisions in the
    actual environment.
  </li>
</ol>

<p>
  The Dyna algorithm combines model-based learning and planning to improve the
  learning speed and efficiency of the standard Q-learning algorithm. By
  incorporating simulated experience, the agent can learn more quickly from its
  experience and make better decisions in the actual environment.
</p>

<h2>Test Environment</h2>
The navigation task takes place in a 10 x 10 grid world. The particular
environment is expressed in a CSV file of integers, where the value in each
position is interpreted as follows:
<ul>
  <li>blank space.</li>
  <li>an obstacle.</li>
  <li>the starting location for the robot.</li>
  <li>the goal location.</li>
  <li>quicksand.</li>
</ul>
